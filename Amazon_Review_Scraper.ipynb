{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intitialize Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for the first page of reviews\n",
    "initial_url = \"https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "\n",
    "# Create text variables for building initial url\n",
    "page_num = '&pageNumber=1'\n",
    "page_size = '&pageSize=50'\n",
    "\n",
    "# Build initial url\n",
    "if page_num not in initial_url:\n",
    "    base_url = initial_url+page_num+page_size\n",
    "else:\n",
    "    base_url = initial_url+page_size\n",
    "\n",
    "# Request url page source to find total number of pages of reviews\n",
    "html = requests.get(base_url).text\n",
    "\n",
    "# Parse to make soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Create list variable page numbers \n",
    "page_numbers = []\n",
    "\n",
    "# Pull page numbers from soup and insert into page_numbers list\n",
    "for li in soup.find_all('li', attrs={'class':'page-button'}):\n",
    "    page_numbers.append(li.text)\n",
    "\n",
    "# Pull last item in page_numbers, which will be the max number of pages of reviews \n",
    "last_page = int(page_numbers[-1].replace(',',''))+1\n",
    "\n",
    "# Create list of 1 through last_page for iterating through in the scraper\n",
    "page = list(range(1, last_page))\n",
    "pages = [str(item) for item in page]\n",
    "\n",
    "# Create reviews list variable, this is where the data pulled from the scraper will be stored\n",
    "# before being inseted into a dataframe\n",
    "reviews = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reveiw Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2615 6.31              50 reviews\n",
      "2706 7.08              50 reviews\n",
      "2714 6.7              50 reviews\n",
      "2724 6.4              50 reviews\n",
      "2757 10.42              50 reviews\n",
      "2804 6.73              50 reviews\n",
      "2826 6.54              50 reviews\n",
      "2865 6.71              50 reviews\n",
      "2887 9.74              50 reviews\n",
      "3623 Error, retrying.\n",
      "3623 Error, retrying.\n",
      "3623 Error, retrying.\n",
      "3623 Error, retrying.\n",
      "3623 47.08              50 reviews\n",
      "3652 7.15              50 reviews\n",
      "3660 Error, retrying.\n",
      "3660 Error, retrying.\n",
      "3660 Error, retrying.\n",
      "3660 Error, retrying.\n",
      "3660 Error, retrying.\n",
      "3660 Error, retrying.\n",
      "3660 Error, retrying.\n",
      "3660 Last try\n",
      "3660 70.88              50 reviews\n",
      "3689 Error, retrying.\n",
      "3689 16.0              50 reviews\n",
      "3710 Error, retrying.\n",
      "3710 Error, retrying.\n",
      "3710 24.87              50 reviews\n",
      "3757 8.1              50 reviews\n",
      "3758 Error, retrying.\n",
      "3758 Error, retrying.\n",
      "3758 Error, retrying.\n",
      "3758 Error, retrying.\n",
      "3758 Error, retrying.\n",
      "3758 Error, retrying.\n",
      "3758 62.36              50 reviews\n",
      "3763 Error, retrying.\n",
      "3763 Error, retrying.\n",
      "3763 27.87              50 reviews\n",
      "Total time elapsed 331.19\n"
     ]
    }
   ],
   "source": [
    "# Start timer for total time scraper takes\n",
    "t0 = time.time()\n",
    "\n",
    "# Loop through to scrape each page in pages \n",
    "for num in pages:\n",
    "    \n",
    "    # Start timer for time taken to scrape page\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Create url for current page to scrape\n",
    "    url = re.sub('pageNumber=\\d+', 'pageNumber='+num, base_url)\n",
    "    \n",
    "\n",
    "    # Request url page source\n",
    "    html = requests.get(url).text\n",
    "\n",
    "    # Parse to make soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Create new soup element for just the section of html that has review data\n",
    "    review_box = soup.find('div', attrs={'id':'cm_cr-review_list'})\n",
    "\n",
    "    # Convert soup object to string for getting review ids\n",
    "    review_box_str = str(review_box)\n",
    "\n",
    "    # Get and construct list of review ids\n",
    "    review_id_pattern = 'customer_review-[A-Z0-9]+'\n",
    "    review_id_string = list(set(re.findall(review_id_pattern, review_box_str)))\n",
    "    review_ids = [item[item.find('R'):] for item in review_id_string]\n",
    "    \n",
    "    # Set trys counter to zero for following while loop\n",
    "    trys = 0\n",
    "    \n",
    "    # If initial html request comes back without review data try request again\n",
    "    # Repeated till data is succesfully gathered or has tried 7 times\n",
    "    while len(review_ids) == 0:\n",
    "        \n",
    "        # Print current page and error message\n",
    "        print(num, 'Error, retrying.')\n",
    "        \n",
    "        # Inset wait time between loops to give the internet a chance to speed up\n",
    "        sleep(2.2)\n",
    "\n",
    "        # Request url page source\n",
    "        html = requests.get(url).text\n",
    "\n",
    "        # Parse to make soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Create new soup element for just the section of html that has review data\n",
    "        review_box = soup.find('div', attrs={'id':'cm_cr-review_list'})\n",
    "\n",
    "        # Convert soup object to string for getting review ids\n",
    "        review_box_str = str(review_box)\n",
    "\n",
    "        # Get and construct list of review ids\n",
    "        review_id_pattern = 'customer_review-[A-Z0-9]+'\n",
    "        review_id_string = list(set(re.findall(review_id_pattern, review_box_str)))\n",
    "        review_ids = [item[item.find('R'):] for item in review_id_string]\n",
    "        \n",
    "        # Add try to trys\n",
    "        trys += 1\n",
    "        \n",
    "        # If statement to keep loop from running for forever\n",
    "        if trys == 7:\n",
    "            \n",
    "            # Print current page number and that this was the last try for this page,\n",
    "            # whether this attempt was succesful or not.\n",
    "            print(num, 'Last try')\n",
    "            \n",
    "            # End while loop\n",
    "            break \n",
    "\n",
    "    # Set iteration counter for the following for loop\n",
    "    iteration = 0\n",
    "\n",
    "    # loop through to get data for each review in review_ids\n",
    "    for i in review_ids:\n",
    "        individual_review = review_box.find('div', attrs={'id':i})\n",
    "\n",
    "        # create row list variable to store attributes of review\n",
    "        row = []\n",
    "\n",
    "        # Get review id for current review\n",
    "        row.append(review_ids[iteration])\n",
    "        \n",
    "        # Get page number for current page\n",
    "        row.append(num)\n",
    "\n",
    "        # Get date of review\n",
    "        row.append(individual_review.find('span', attrs={'data-hook':'review-date'}).text)\n",
    "\n",
    "        # Get title of review\n",
    "        row.append(individual_review.find('a', attrs={'class':'a-size-base a-link-normal review-title a-color-base a-text-bold'}).text)\n",
    "\n",
    "        # Get rating of review\n",
    "        rating_pattern = '\\d.\\d out of 5 stars'\n",
    "        rating_string = re.match(rating_pattern, individual_review.text).group(0)\n",
    "        row.append(rating_string)\n",
    "\n",
    "        # Get scalar boolean of whther or not reviewer is varified purchaser\n",
    "        row.append(len(individual_review.find_all('span', attrs={'data-hook':'avp-badge'})))\n",
    "\n",
    "        # Get reveiw helpfulness rating\n",
    "        if individual_review.find('span', attrs={'data-hook':'helpful-vote-statement'}) == None:\n",
    "            row.append(\"0\")\n",
    "        elif (individual_review.find('span', attrs={'data-hook':'helpful-vote-statement'}).text == \"One person found this helpful\"):\n",
    "            row.append(\"1\")\n",
    "        else:\n",
    "            row.append(individual_review.find('span', attrs={'data-hook':'helpful-vote-statement'}).text[:-26])\n",
    "\n",
    "        # Get body of review\n",
    "        row.append(individual_review.find('span', attrs={'data-hook':'review-body'}).text)\n",
    "        \n",
    "        # Store url\n",
    "        row.append(url)\n",
    "\n",
    "        # Store review row in reviews\n",
    "        reviews.append(row)\n",
    "        \n",
    "        # Add 1 to iteration counter\n",
    "        iteration += 1\n",
    "        \n",
    "    # Inset wait time between loops to avoid over burdening Amazon's servers\n",
    "    sleep(1.2)\n",
    "    \n",
    "    # End timer for time taken to scrape page\n",
    "    t3= time.time()\n",
    "    \n",
    "    # Print \n",
    "    print(num, round(t3 - t2,2), '            ', len(review_ids), 'reviews')\n",
    "\n",
    "# Create column names list variable\n",
    "column_names = 'id page_number date title rating varified_purchase found_helpful body url'.split()\n",
    "    \n",
    "# Creat dataframe from scraped data and save as csv\n",
    "pd.DataFrame(reviews, columns = column_names).to_csv('Reviews_5.9th_attempt.csv')\n",
    "\n",
    "# \n",
    "t1 = time.time()\n",
    "print('Total time elapsed', round(t1-t0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This section is for grabing whatever pages were missed in the initial scrape\n",
    "\n",
    "# Import review csv\n",
    "df = pd.read_csv('Reviews_5.9th_attempt.csv', index_col = 0, dtype = {'found_helpful':object})\n",
    "\n",
    "# Data cleaning for attempt 4 and on \n",
    "\n",
    "# Get rid of duplicate reviews\n",
    "df.drop_duplicates(['id'], inplace = True)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Make list of page numbers contained in df\n",
    "df_page = list(df['page_number'].unique())\n",
    "\n",
    "# Convert df_page numbers to list of strings\n",
    "df_pages = [str(item) for item in df_page]\n",
    "\n",
    "# Update pages variable to only contain pages that haver not been successfully retrieved\n",
    "pages = [x for x in pages if x not in df_pages]\n",
    "\n",
    "# Number of pages that were missed\n",
    "len(pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
